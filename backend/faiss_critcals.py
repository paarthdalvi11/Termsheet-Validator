# -*- coding: utf-8 -*-
"""FAISS critcals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ztn8iDDItjXSX2QWSTGWUFiCdCCkV8It
"""



# import json
# import numpy as np
# import faiss
# from sentence_transformers import SentenceTransformer

# # Load model for encoding query
# model = SentenceTransformer("all-MiniLM-L6-v2")  # You can replace with a better one like 'mistral' if needed

# # Function to detect critical chunks from JSON
# def detect_critical_chunks(json_path, top_k=10):
#     with open(json_path, "r") as f:
#         data = json.load(f)

#     texts = [item["content"] for item in data]
#     ids = [item["id"] for item in data]
#     vectors = np.array([item["vector"] for item in data]).astype("float32")

#     # Build FAISS index
#     dim = len(vectors[0])
#     index = faiss.IndexFlatL2(dim)
#     index.add(vectors)

#     # Define pseudo-critical query
#     critical_query = "interest rate, call option, maturity date, credit rating, redemption, floating rate note, issuer rating"
#     query_vector = model.encode([critical_query]).astype("float32")

#     # Similarity search
#     D, I = index.search(query_vector, top_k)

#     # Extract top-matching chunks
#     critical_chunks = []
#     for rank, idx in enumerate(I[0]):
#         critical_chunks.append({
#             "id": ids[idx],
#             "text": texts[idx],
#             "score": D[0][rank]
#         })

#     return critical_chunks

# #  Run it on your file
# if __name__ == "__main__":
#     json_path = "/content/pdf_chunks_with_vectors.json"  # <- your JSON file here
#     critical_chunks = detect_critical_chunks(json_path, top_k=15)

#     for chunk in critical_chunks:
#         print(f"[Chunk ID: {chunk['id']}] Score: {chunk['score']:.2f}\nText: {chunk['text']}\n---")

import json
import numpy as np # type: ignore
import faiss # type: ignore


# --- Define critical financial clause keywords ---
CRITICAL_KEYWORDS = [
    "Change of Control", "Put Option", "Redemption", "Issuer Call", "Make-Whole",
    "Early Redemption", "Default", "Interest Payment", "Coupon", "Rate(s) of Interest",
    "Floating Rate", "Zero Coupon", "Fixed Rate", "Interest Commencement", "Maturity Date"
]

# --- Check if a chunk is critical ---
def is_critical_clause(text):
    for keyword in CRITICAL_KEYWORDS:
        if keyword.lower() in text.lower():
            return True
    return False

# --- Classify termsheet using FAISS with cosine similarity ---
def classify_termsheet_with_faiss(json_path, query_vector, faiss_index, top_k=5):
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # Ensure query vector is normalized and properly shaped
    query_vector = np.array(query_vector).astype('float32').reshape(1, -1)
    query_vector = query_vector / np.linalg.norm(query_vector, axis=1, keepdims=True)
    
    # For cosine similarity, higher scores are better (unlike L2 distance)
    scores, indices = faiss_index.search(query_vector, top_k)

    matched_chunks = [data[i] for i in indices[0] if i < len(data)]

    critical_chunks = []
    for chunk in matched_chunks:
        content = chunk.get("content", "")
        if is_critical_clause(content):
            critical_chunks.append({
                "chunk_id": chunk["id"],
                "text": content.strip(),
                "similarity_score": float(scores[0][matched_chunks.index(chunk)])
            })

    # Sort critical chunks by similarity score (highest first)
    critical_chunks.sort(key=lambda x: x["similarity_score"], reverse=True)
    
    is_critical = len(critical_chunks) > 0

    return {
        "is_critical": is_critical,
        "critical_chunks": critical_chunks
    }

# --- Prompt for LLM ---
def build_validation_prompt(critical_clauses):
    prompt = """
    The following are critical clauses from a financial termsheet. 
    Validate if the termsheet is valid or not valid.
    
    Provide your response in a SINGLE, COHERENT analysis with this structure:
    1. Validation: valid or not valid
    2. Justification: explain why the termsheet is valid or not valid
    3. One final summary section
    
    Clauses to validate:
    """
    
    for i, clause in enumerate(critical_clauses, 1):
        prompt += f"\n{i}. {clause['text']}\n"
        
    return prompt


# --- Call Ollama + Mistral locally ---
def call_ollama_mistral(prompt):
    import requests
    
    # Use Ollama's REST API (default port is 11434)
    api_url = "http://localhost:11434/api/generate"
    
    # Request payload with model and parameters
    payload = {
        "model": "mistral",
        "prompt": prompt,
        "temperature": 0.1,
        "stream": False  # Get complete response at once
    }
    
    try:
        response = requests.post(api_url, json=payload)
        response.raise_for_status()  # Raise exception for HTTP errors
        
        result = response.json()
        return result.get("response", "No response received")
        
    except requests.exceptions.RequestException as e:
        print(f"Error calling Ollama API: {e}")
        return f"Error: {str(e)}"


# --- MAIN ---
if __name__ == "__main__":
    json_file_path = r"D:\barclays\pdf1_chunks_with_vectors.json"

    # Build FAISS index with cosine similarity
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    dim = len(data[0]["vector"])
    vectors = np.array([chunk["vector"] for chunk in data]).astype('float32')
    
    # Normalize vectors for cosine similarity
    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
    
    # Use IndexFlatIP (Inner Product) for cosine similarity instead of IndexFlatL2
    index = faiss.IndexFlatIP(dim)
    index.add(vectors)

    # Use a consistent query for reproducible results
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # Set a seed for reproducibility
    np.random.seed(42)
    
    # Use a more specific query related to financial termsheets
    query_text = "barclays structured products termsheet validation regulatory compliance interest calculation maturity conditions redemption provisions risk assessment default clauses payment terms reference assets"
    query_vector = model.encode(query_text, normalize_embeddings=True)

    result = classify_termsheet_with_faiss(json_file_path, query_vector, index)

    print(f"Termsheet Critical: {result['is_critical']}")
    if result["is_critical"]:
        print("Critical Clauses:")
        for chunk in result["critical_chunks"]:
            print(f"- Chunk ID: {chunk['chunk_id']}")
            print(f"  Similarity Score: {chunk['similarity_score']:.4f}")
            print(f"  Text: {chunk['text']}\n")

    # Save result
    with open("critical_result.json", "w", encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    # Validate with Ollama if critical
    if result["is_critical"]:
        print("Termsheet flagged as CRITICAL. Validating with LLM...\n")
        prompt = build_validation_prompt(result["critical_chunks"])
        response = call_ollama_mistral(prompt)
        print("\n--- LLM Validation Response ---")
        print(response)
    else:
        print("No critical clauses found. Skipping validation.")
